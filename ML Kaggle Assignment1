import pandas as pd  
import numpy as np  
import matplotlib.pyplot as plt  
import seaborn as seabornInstance 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
from sklearn import metrics
%matplotlib inline
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVR
from sklearn.model_selection import KFold 

import matplotlib.pyplot as plt ##for plotting
import seaborn as sns           
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
Train = pd.read_csv("/Users/BarryFitzpatrick/Downloads/tcd ml 2019-20 income prediction training (with labels).csv")
Test1 = pd.read_csv("/Users/BarryFitzpatrick/Downloads/tcd ml 2019-20 income prediction test (without labels).csv")

plt.scatter(Train["Age"], Train["Income in EUR"])

Train['train']=1
Test1['train']=0
combined = pd.concat([Train, Test1], ignore_index=True, sort=False)

df=pd.get_dummies(data=combined, columns=['Country','Profession']) #one-hot encoding for countries & professions

Train = df[df['train'] ==1]
Test1 = df[df['train'] ==0]

Train.drop(["train"],axis=1,inplace=True)
Test1.drop(["train"],axis=1,inplace=True)

sns.boxplot(x = Train["Income in EUR"]) #It is clearenough that anything >3m is outlier

#Remove outliers above 3m
Train =  Train[Train['Income in EUR']<=3000000 ]

sns.boxplot(x = Train["Income in EUR"])

plt.scatter(Train["Size of City"], Train["Income in EUR"]) 
#two clear outliers, between 2-3m ppl, >1m income

# delete 2 clear outliers spotted
indexSizeCityOutliers = Train[ (Train["Size of City"] >= 20000000)  & (Train["Income in EUR"] > 1000000) ].index
indexSizeCityOutliers

Train = Train.drop(indexSizeCityOutliers)

sns.boxplot(x = Train["Size of City"])
#Clearly two seperate distributions here. Will make categories on this

#Test["CitySize"]>7335190 = 1
Train['BigCity'] = np.where(Train['Size of City']>7335190, 1, 0)
Train['SmallCity'] = np.where(Train['Size of City']<7335190, 1, 0)

#Train = Train.drop(columns=['Size of City']) #have re allocated these values above

Train.isnull().any() #missing values? yes or no

Train = Train.fillna(method='ffill') 
Train = pd.get_dummies(Train, columns=["Hair Color"]) #OHEnc Hair
Train = pd.get_dummies(Train, columns=["University Degree"])
Train = pd.get_dummies(Train, columns=['Gender'])
Train = Train.drop(columns=['Instance']) #Instance is useless

X=Train.columns
X=X.drop('Income in EUR')

X=Train[X].values
y = Train["Income in EUR"].values


######### Attempts at other regression models #########
#from sklearn.model_selection import RepeatedKFold
#from sklearn.ensemble import RandomForestRegressor 
  
 # create regressor object 
#regressor = RandomForestRegressor(n_estimators = 100, random_state = 0) 
  
# fit the regressor with x and y data 
#regressor.fit(X, y) 

#from sklearn.model_selection import GridSearchCV
#from sklearn.linear_model import Ridge

#ridge=Ridge()

#parameters={'alpha':[1e-10,1e-5,1e-2,1,5,10]}

#ridge_regressor=GridSearchCV(ridge,parameters,scoring='neg_mean_squared_error', cv=5)

#ridge_regressor.fit(X,y)

#from sklearn.linear_model import Lasso
#lasso_reg = Lasso(normalize=True)

#Fitting the Training data to the Lasso regressor
#lasso_reg.fit(X,y)

#kf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None) 

#for train_index, test_index in kf.split(X):
#      print("Train:", train_index, "Validation:",test_index)
#      X_train, X_test = X[train_index], X[test_index] 
#      y_train, y_test = y[train_index], y[test_index]

#from sklearn.linear_model import Ridge
#def ridge_regression(data, predictors, alpha, models_to_plot={}):
    #Fit the model
 #   ridgereg = Ridge(alpha=alpha,normalize=True)
  #  ridgereg.fit(X,y)
   # y_pred = ridgereg.predict(X_test)
   
 ######### Ridge worked reasonably ok... ####
#rr = Ridge(alpha=0.01)    #81335
#rr = Ridge(alpha=10)       #83092
#rr = Ridge(alpha=1e-10)    #81353
#rr = Ridge(alpha=100)      #90k
#rr = Ridge(alpha=10000)    #140k
#rr = Ridge(alpha=1e-4)
#rr.fit(X_train, y_train) 
#y_pred= rr.predict(X_test)

###### Random forest was too inaccurate, unable to run for large number of trees###
# Import the model we are using
#from sklearn.ensemble import RandomForestRegressor
#rf = RandomForestRegressor(n_estimators = 100, random_state = 42)
# Train the model on training data
#rf.fit(X_train, y_train);

#y_pred = rf.predict(X_test)

#y_pred =lasso_reg.predict(X_test)

#########################

# Model Used ##
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

regressor = LinearRegression()  
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df1 = df.head(25)

df1.plot(kind='bar',figsize=(10,8))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))


#Test["CitySize"]>7335190 = 1
Test1['BigCity'] = np.where(Test1['Size of City']>7335190, 1, 0)
Test1['SmallCity'] = np.where(Test1['Size of City']<7335190, 1, 0)

Test1 = Test1.drop(columns=['Instance'])
Test1 = pd.get_dummies(Test1, columns=["Hair Color"])
Test1 = pd.get_dummies(Test1, columns=["University Degree"])
Test1 = pd.get_dummies(Test1, columns=["Gender"])
Test1 = Test1.fillna(method='ffill')
Xx=Test1.columns
Xx=Xx.drop('Income in EUR')
Xx=Test1[Xx].values
Pred = regressor.predict(Xx) #prediction
#Pred =lasso_reg.predict(Xx)
#Pred= rr.predict(Xx)

import numpy
numpy.savetxt("kaggle2.csv", Pred, delimiter=",") #export and save file




















